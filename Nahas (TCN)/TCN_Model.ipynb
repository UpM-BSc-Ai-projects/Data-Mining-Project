{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5e557d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff07c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b67e510",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d57c72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target column: Close\n",
      "Epoch 1/50, Train Loss: 0.0001, Val Loss: 0.0005\n",
      "Epoch 2/50, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 3/50, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 4/50, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 5/50, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 6/50, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 7/50, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 8/50, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 9/50, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 10/50, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 11/50, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 12/50, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 13/50, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 14/50, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 15/50, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 16/50, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 17/50, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 18/50, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 19/50, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 20/50, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 21/50, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 22/50, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 23/50, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 24/50, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 25/50, Train Loss: 0.0000, Val Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"preprocessed_hourly_data.csv\")\n",
    "target_column = 'Close'\n",
    "\n",
    "# Use all columns except datetime and target as features\n",
    "features = [col for col in data.columns if col not in ['datetime', target_column]]\n",
    "\n",
    "# Chronological split\n",
    "train_size = int(0.65 * len(data))\n",
    "val_size = int(0.15 * len(data))\n",
    "test_size = len(data) - train_size - val_size\n",
    "\n",
    "train_data = data.iloc[:train_size]\n",
    "val_data = data.iloc[train_size:train_size+val_size]\n",
    "test_data = data.iloc[train_size+val_size:]\n",
    "\n",
    "# Sequence creation\n",
    "def create_sequences(features, target, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(features) - seq_length):\n",
    "        X.append(features[i:i+seq_length])\n",
    "        y.append(target[i+seq_length])\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "seq_length = 24\n",
    "X_train, y_train = create_sequences(train_data[features].values, train_data[target_column].values, seq_length)\n",
    "X_val, y_val = create_sequences(val_data[features].values, val_data[target_column].values, seq_length)\n",
    "X_test, y_test = create_sequences(test_data[features].values, test_data[target_column].values, seq_length)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_val = torch.tensor(X_val).float()\n",
    "y_val = torch.tensor(y_val).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "# DataLoader setup\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, output_size=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Initialize weights properly\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.ones_(param)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.ones_(self.fc.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, seq_len, features]\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Use only the last output\n",
    "        return self.fc(lstm_out[:, -1, :])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleLSTM(input_size=len(features)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.ReduceLROnPlateau(optimizer, patience=5)\n",
    "criterion = nn.HuberLoss()\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * len(X_batch)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            val_loss += criterion(outputs, y_batch).item() * len(X_batch)\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
    "\n",
    "# Evaluation Fix\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch).cpu().numpy().flatten()\n",
    "        predictions.extend(outputs)\n",
    "        actuals.extend(y_batch.numpy().flatten())\n",
    "\n",
    "# Critical Fix: Proper Alignment\n",
    "test_prices = data[target_column].values[train_size+val_size:]\n",
    "naive_pred = test_prices[:-1]  # Previous prices\n",
    "predictions = predictions[1:]  # Align with naive baseline\n",
    "actuals = actuals[1:]  # Remove first prediction\n",
    "\n",
    "# Final Length Check\n",
    "min_length = min(len(actuals), len(predictions), len(naive_pred))\n",
    "actuals = actuals[:min_length]\n",
    "predictions = predictions[:min_length]\n",
    "naive_pred = naive_pred[:min_length]\n",
    "\n",
    "# Metrics\n",
    "mse = mean_squared_error(actuals, predictions)\n",
    "huber = criterion(torch.tensor(predictions), torch.tensor(actuals)).item()\n",
    "naive_mse = mean_squared_error(actuals, naive_pred)\n",
    "\n",
    "print(f\"\\nTest MSE: {mse:.2f}\")\n",
    "print(f\"Test Huber: {huber:.2f}\")\n",
    "print(f\"Naive Baseline MSE: {naive_mse:.2f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(actuals[:200], label='Actual Prices')\n",
    "plt.plot(predictions[:200], label='Predicted Prices')\n",
    "plt.plot(naive_pred[:200], label='Naive Baseline', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title(\"Price Predictions vs Actuals (First 200 Samples)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b54db00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
